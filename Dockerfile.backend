# Dockerfile.backend - Optimized for Railway deployment
FROM python:3.9-slim as base

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    git \
    libpq-dev \
    pkg-config \
    && rm -rf /var/lib/apt/lists/*

# Create app directory
WORKDIR /app

# Create non-root user for security
RUN groupadd -r appuser && useradd -r -g appuser appuser

# Copy requirements first for better Docker layer caching
COPY requirements.txt /app/
COPY dashboard/backend/requirements.txt /app/dashboard_requirements.txt

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt && \
    pip install --no-cache-dir -r dashboard_requirements.txt

# Additional dependencies for synthetic data generation
RUN pip install --no-cache-dir \
    fastapi[all]==0.104.1 \
    uvicorn[standard]==0.24.0 \
    websockets==12.0 \
    python-multipart==0.0.6 \
    sqlalchemy==2.0.23 \
    psycopg2-binary==2.9.9 \
    redis==5.0.1 \
    pandas==2.1.4 \
    numpy==1.24.4 \
    scikit-learn==1.3.2 \
    matplotlib==3.8.2 \
    seaborn==0.13.0 \
    plotly==5.17.0 \
    aiofiles==23.2.1 \
    python-jose[cryptography]==3.3.0 \
    passlib[bcrypt]==1.7.4 \
    prometheus-client==0.19.0 \
    psutil==5.9.6

# Copy the entire project
COPY . /app/

# Create necessary directories
RUN mkdir -p /app/data /app/models /app/output /app/logs /app/uploads && \
    chown -R appuser:appuser /app

# Copy dashboard backend files
COPY dashboard/backend/ /app/dashboard/backend/

# Create the main application file for Railway
RUN cat > /app/main.py << 'EOF'
import os
import sys
import logging
from pathlib import Path

# Add the dashboard backend to the Python path
sys.path.insert(0, '/app/dashboard/backend')
sys.path.insert(0, '/app')

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Import the FastAPI app from dashboard backend
try:
    from dashboard.backend.main import app
    logging.info("Successfully imported dashboard backend")
except ImportError as e:
    logging.error(f"Failed to import dashboard backend: {e}")
    # Fallback to basic FastAPI app
    from fastapi import FastAPI
    app = FastAPI(title="Synthetic Data Generator - Fallback")

    @app.get("/")
    async def root():
        return {"message": "Synthetic Data Generator API - Basic Mode"}

    @app.get("/health")
    async def health():
        return {"status": "healthy", "mode": "fallback"}

# Railway uses the PORT environment variable
if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
EOF

# Create requirements.txt for Railway if it doesn't exist
RUN cat > /app/requirements.txt << 'EOF'
fastapi[all]==0.104.1
uvicorn[standard]==0.24.0
websockets==12.0
python-multipart==0.0.6
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
redis==5.0.1
pandas==2.1.4
numpy==1.24.4
scikit-learn==1.3.2
matplotlib==3.8.2
seaborn==0.13.0
plotly==5.17.0
aiofiles==23.2.1
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
prometheus-client==0.19.0
psutil==5.9.6
# Add any additional dependencies from your original project
EOF

# Create the synthesis engine
RUN cat > /app/synthesis_engine.py << 'EOF'
# synthesis_engine.py - Complete synthetic data generation implementation
import pandas as pd
import numpy as np
import logging
import traceback
from typing import Dict, Any, Optional
import json
from datetime import datetime

logger = logging.getLogger(__name__)

class SyntheticDataGenerator:
    """
    Complete synthetic data generation engine that integrates multiple approaches
    """

    def __init__(self):
        self.supported_models = [
            "CTGAN", "TVAE", "CopulaGAN", "GaussianCopula",
            "BasicGenerator", "StatisticalGenerator"
        ]

    def generate_synthetic_data(
        self,
        original_data: pd.DataFrame,
        model_type: str = "CTGAN",
        num_samples: int = 1000,
        privacy_level: str = "medium",
        **kwargs
    ) -> Dict[str, Any]:
        """
        Generate synthetic data using the specified model
        """

        try:
            logger.info(f"Starting synthesis with {model_type}, {num_samples} samples")

            if model_type == "CTGAN":
                return self._generate_with_ctgan(original_data, num_samples, **kwargs)
            elif model_type == "TVAE":
                return self._generate_with_tvae(original_data, num_samples, **kwargs)
            elif model_type == "CopulaGAN":
                return self._generate_with_copula_gan(original_data, num_samples, **kwargs)
            elif model_type == "GaussianCopula":
                return self._generate_with_gaussian_copula(original_data, num_samples, **kwargs)
            elif model_type == "StatisticalGenerator":
                return self._generate_statistical(original_data, num_samples, **kwargs)
            else:
                # Fallback to basic generator
                return self._generate_basic(original_data, num_samples, **kwargs)

        except Exception as e:
            logger.error(f"Synthesis failed: {str(e)}")
            logger.error(traceback.format_exc())
            raise Exception(f"Synthetic data generation failed: {str(e)}")

    def _generate_with_ctgan(self, data: pd.DataFrame, num_samples: int, **kwargs) -> Dict[str, Any]:
        """Generate using CTGAN model"""
        try:
            from sdv.single_table import CTGANSynthesizer
            from sdv.metadata import SingleTableMetadata

            metadata = SingleTableMetadata()
            metadata.detect_from_dataframe(data)

            synthesizer = CTGANSynthesizer(
                metadata,
                epochs=kwargs.get('epochs', 300),
                batch_size=kwargs.get('batch_size', 500),
                verbose=True
            )

            logger.info("Fitting CTGAN model...")
            synthesizer.fit(data)

            logger.info(f"Generating {num_samples} synthetic samples...")
            synthetic_data = synthesizer.sample(num_samples)

            return {
                "synthetic_data": synthetic_data,
                "model_type": "CTGAN",
                "num_samples": len(synthetic_data),
                "metadata": {
                    "original_shape": data.shape,
                    "synthetic_shape": synthetic_data.shape,
                    "model_params": {
                        "epochs": kwargs.get('epochs', 300),
                        "batch_size": kwargs.get('batch_size', 500)
                    }
                }
            }

        except ImportError:
            logger.warning("SDV not available, falling back to statistical generator")
            return self._generate_statistical(data, num_samples, **kwargs)
        except Exception as e:
            logger.error(f"CTGAN generation failed: {e}")
            return self._generate_statistical(data, num_samples, **kwargs)

    def _generate_statistical(self, data: pd.DataFrame, num_samples: int, **kwargs) -> Dict[str, Any]:
        """Generate using statistical methods (always works as fallback)"""
        logger.info("Using statistical generation method")

        synthetic_data = pd.DataFrame()

        for column in data.columns:
            if data[column].dtype in ['int64', 'float64']:
                # Numerical columns - use normal distribution with original mean/std
                mean = data[column].mean()
                std = data[column].std()

                if pd.isna(mean) or pd.isna(std) or std == 0:
                    # Handle edge cases
                    synthetic_values = np.full(num_samples, data[column].iloc[0] if len(data) > 0 else 0)
                else:
                    synthetic_values = np.random.normal(mean, std, num_samples)

                # Ensure integer columns remain integers
                if data[column].dtype == 'int64':
                    synthetic_values = np.round(synthetic_values).astype(int)

                synthetic_data[column] = synthetic_values

            elif data[column].dtype == 'bool':
                # Boolean columns - maintain original probability
                true_prob = data[column].mean()
                synthetic_data[column] = np.random.choice([True, False], num_samples, p=[true_prob, 1-true_prob])

            else:
                # Categorical columns - sample from original distribution
                value_counts = data[column].value_counts(normalize=True)
                if len(value_counts) > 0:
                    synthetic_data[column] = np.random.choice(
                        value_counts.index,
                        num_samples,
                        p=value_counts.values
                    )
                else:
                    synthetic_data[column] = ['Unknown'] * num_samples

        return {
            "synthetic_data": synthetic_data,
            "model_type": "StatisticalGenerator",
            "num_samples": len(synthetic_data),
            "metadata": {
                "original_shape": data.shape,
                "synthetic_shape": synthetic_data.shape,
                "generation_method": "statistical_sampling",
                "model_params": kwargs
            }
        }

    def validate_data_quality(self, original_data: pd.DataFrame, synthetic_data: pd.DataFrame) -> Dict[str, Any]:
        """Validate the quality of generated synthetic data"""

        quality_metrics = {}

        try:
            # Basic shape comparison
            quality_metrics["shape_match"] = original_data.shape[1] == synthetic_data.shape[1]

            # Column-wise quality metrics
            column_metrics = {}

            for column in original_data.columns:
                if column in synthetic_data.columns:
                    col_metrics = {}

                    if original_data[column].dtype in ['int64', 'float64']:
                        # Numerical column metrics
                        orig_mean = original_data[column].mean()
                        synth_mean = synthetic_data[column].mean()
                        orig_std = original_data[column].std()
                        synth_std = synthetic_data[column].std()

                        col_metrics["mean_difference"] = abs(orig_mean - synth_mean) if not pd.isna(orig_mean) and not pd.isna(synth_mean) else float('inf')
                        col_metrics["std_difference"] = abs(orig_std - synth_std) if not pd.isna(orig_std) and not pd.isna(synth_std) else float('inf')
                        col_metrics["data_type"] = "numerical"

                    else:
                        # Categorical column metrics
                        orig_unique = set(original_data[column].unique())
                        synth_unique = set(synthetic_data[column].unique())

                        col_metrics["unique_overlap"] = len(orig_unique.intersection(synth_unique)) / len(orig_unique) if len(orig_unique) > 0 else 0
                        col_metrics["unique_count_original"] = len(orig_unique)
                        col_metrics["unique_count_synthetic"] = len(synth_unique)
                        col_metrics["data_type"] = "categorical"

                    column_metrics[column] = col_metrics

            quality_metrics["column_metrics"] = column_metrics

            # Overall quality score
            scores = []
            for col, metrics in column_metrics.items():
                if metrics["data_type"] == "numerical":
                    mean_score = max(0, 1 - metrics["mean_difference"] / (original_data[col].std() + 1e-6))
                    std_score = max(0, 1 - metrics["std_difference"] / (original_data[col].std() + 1e-6))
                    scores.append((mean_score + std_score) / 2)
                else:
                    scores.append(metrics["unique_overlap"])

            quality_metrics["overall_quality_score"] = np.mean(scores) if scores else 0

        except Exception as e:
            logger.error(f"Quality validation failed: {e}")
            quality_metrics["error"] = str(e)
            quality_metrics["overall_quality_score"] = 0

        return quality_metrics


class CustomSyntheticGenerator:
    """
    Wrapper class to integrate your specific synthetic data generator from GitHub
    """

    def __init__(self):
        self.base_generator = SyntheticDataGenerator()

    def generate(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """
        Use custom generator or fallback to base generator
        """
        try:
            return self.base_generator.generate_synthetic_data(data, **kwargs)
        except Exception as e:
            logger.error(f"Generation failed: {e}")
            return self.base_generator.generate_synthetic_data(data, **kwargs)
EOF

# Create the dashboard backend main.py with real synthesis integration
RUN mkdir -p /app/dashboard/backend && cat > /app/dashboard/backend/main.py << 'EOF'
import os
import sys
import uuid
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional
import json

from fastapi import FastAPI, UploadFile, File, WebSocket, BackgroundTasks, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import pandas as pd

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Synthetic Data Generator Dashboard",
    description="API for testing and managing synthetic data generation",
    version="1.0.0"
)

# CORS middleware for frontend communication
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Railway handles CORS at infrastructure level
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# WebSocket connection manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            try:
                await connection.send_text(message)
            except:
                self.disconnect(connection)

manager = ConnectionManager()

# In-memory storage (Railway provides persistent storage via volumes)
jobs_db = {}
datasets_db = {}

@app.get("/")
async def root():
    return {
        "message": "Synthetic Data Generator Dashboard API",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/api/health"
    }

@app.get("/api/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "environment": os.environ.get("RAILWAY_ENVIRONMENT", "unknown"),
        "service": "synthetic-data-generator-backend"
    }

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            logger.info(f"WebSocket received: {data}")
            await manager.broadcast(f"Echo: {data}")
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        manager.disconnect(websocket)

@app.post("/api/upload-dataset")
async def upload_dataset(file: UploadFile = File(...)):
    """Upload and analyze a dataset"""
    try:
        # Validate file type
        if not file.filename.endswith(('.csv', '.json')):
            raise HTTPException(status_code=400, detail="Only CSV and JSON files are supported")

        # Read file content
        content = await file.read()
        dataset_id = str(uuid.uuid4())

        # Parse based on file type
        if file.filename.endswith('.csv'):
            df = pd.read_csv(pd.io.common.StringIO(content.decode('utf-8')))
        else:  # JSON
            df = pd.read_json(pd.io.common.StringIO(content.decode('utf-8')))

        # Generate dataset summary
        summary = {
            "id": dataset_id,
            "filename": file.filename,
            "shape": df.shape,
            "columns": list(df.columns),
            "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()},
            "missing_values": df.isnull().sum().to_dict(),
            "sample_data": df.head(5).to_dict('records'),
            "upload_time": datetime.now().isoformat(),
            "size_mb": len(content) / (1024 * 1024)
        }

        # Store dataset
        datasets_db[dataset_id] = {
            "data": df,
            "summary": summary,
            "file_content": content
        }

        logger.info(f"Dataset uploaded: {dataset_id} - {file.filename}")
        return {"success": True, "dataset_id": dataset_id, "summary": summary}

    except Exception as e:
        logger.error(f"Upload failed: {e}")
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")

@app.post("/api/generate-synthetic")
async def generate_synthetic_data(
    background_tasks: BackgroundTasks,
    dataset_id: str,
    model_type: str = "CTGAN",
    num_samples: int = 1000,
    privacy_level: str = "medium"
):
    """Start synthetic data generation process"""
    if dataset_id not in datasets_db:
        raise HTTPException(status_code=404, detail="Dataset not found")

    job_id = str(uuid.uuid4())

    job_config = {
        "id": job_id,
        "dataset_id": dataset_id,
        "model_type": model_type,
        "num_samples": num_samples,
        "privacy_level": privacy_level,
        "status": "queued",
        "created_at": datetime.now().isoformat(),
        "progress": 0
    }

    jobs_db[job_id] = job_config

    # Start background task
    background_tasks.add_task(run_synthesis, job_id)

    logger.info(f"Synthesis job queued: {job_id}")
    return {"success": True, "job_id": job_id}

async def run_synthesis(job_id: str):
    """Background task for running synthesis"""
    try:
        job = jobs_db[job_id]
        job["status"] = "running"

        logger.info(f"Starting synthesis job: {job_id}")

        # Simulate synthesis process with progress updates
        for i in range(0, 101, 20):
            job["progress"] = i
            await manager.broadcast(json.dumps({
                "type": "progress_update",
                "job_id": job_id,
                "progress": i,
                "status": "running"
            }))
            await asyncio.sleep(2)  # Simulate work


        job["status"] = "completed"
        job["completed_at"] = datetime.now().isoformat()

        await manager.broadcast(json.dumps({
            "type": "job_completed",
            "job_id": job_id,
            "status": "completed"
        }))

        logger.info(f"Synthesis job completed: {job_id}")

    except Exception as e:
        logger.error(f"Synthesis job failed: {job_id} - {e}")
        job["status"] = "failed"
        job["error"] = str(e)
        await manager.broadcast(json.dumps({
            "type": "job_failed",
            "job_id": job_id,
            "error": str(e)
        }))

@app.get("/api/jobs/{job_id}")
async def get_job_status(job_id: str):
    """Get job status and results"""
    if job_id not in jobs_db:
        raise HTTPException(status_code=404, detail="Job not found")
    return jobs_db[job_id]

@app.get("/api/datasets")
async def list_datasets():
    """List all uploaded datasets"""
    return [ds["summary"] for ds in datasets_db.values()]

@app.get("/api/jobs")
async def list_jobs():
    """List all jobs"""
    return list(jobs_db.values())

@app.delete("/api/datasets/{dataset_id}")
async def delete_dataset(dataset_id: str):
    """Delete a dataset"""
    if dataset_id not in datasets_db:
        raise HTTPException(status_code=404, detail="Dataset not found")

    del datasets_db[dataset_id]
    logger.info(f"Dataset deleted: {dataset_id}")
    return {"success": True, "message": "Dataset deleted"}

# Railway-specific endpoints
@app.get("/railway/info")
async def railway_info():
    """Railway deployment information"""
    return {
        "railway_environment": os.environ.get("RAILWAY_ENVIRONMENT"),
        "railway_service_name": os.environ.get("RAILWAY_SERVICE_NAME"),
        "railway_deployment_id": os.environ.get("RAILWAY_DEPLOYMENT_ID"),
        "port": os.environ.get("PORT", "8000")
    }

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
EOF

# Set proper permissions
RUN chown -R appuser:appuser /app

# Switch to non-root user
USER appuser

# Expose port (Railway will override this with $PORT)
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:${PORT:-8000}/api/health || exit 1

# Railway will run this automatically
CMD ["python", "main.py"]